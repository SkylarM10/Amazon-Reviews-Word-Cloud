{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ff6677d-12b7-4974-9930-c994640f16ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the URL:  https://www.amazon.com/dp/1616953675/ref=s9_acsd_al_bw_c2_x_6_i?pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-14&pf_rd_r=BN5NXPXRVXMD33K837AH&pf_rd_t=101&pf_rd_p=4174c67c-49fa-4b31-a362-5bd7da5e3c6a&pf_rd_i=283155\n"
     ]
    }
   ],
   "source": [
    "# Take user input for the url and get the review list url\n",
    "import re\n",
    "\n",
    "#Getting user input\n",
    "url = input(\"Enter the URL: \")\n",
    "\n",
    "# Splitting the url and creating the link to the reviews list\n",
    "url_string = re.split('/dp/', url)\n",
    "product = url_string[0]\n",
    "product_id = re.split(r'\\?', url_string[1])[0] #Note that we can't do '?' since ? is an expression in re\n",
    "review_url = product + '/product-reviews/' + product_id + '/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0606d77d-15da-4ea3-b65e-e5e97ebd62d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from requests_html import AsyncHTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "session = AsyncHTMLSession()\n",
    "review_data = []\n",
    "\n",
    "while review_url:\n",
    "    r = await session.get(review_url)\n",
    "    await r.html.arender(sleep=5)\n",
    "    soup = BeautifulSoup(r.html.html, 'lxml')\n",
    "    reviews = soup.find_all(\"div\", {\"data-hook\": \"review\"})\n",
    "    \n",
    "    for x in reviews:\n",
    "        if x:\n",
    "            title_element = x.find('a', {'data-hook': 'review-title'})\n",
    "            body_element = x.find('span', {'data-hook': 'review-body'})\n",
    "\n",
    "            title = title_element.text.strip() if title_element else ''\n",
    "            body = body_element.text.strip() if body_element else ''\n",
    "\n",
    "            review = {\n",
    "                'title': title,\n",
    "                'body': body\n",
    "            }\n",
    "            review_data.append(review)\n",
    "\n",
    "    next_page_element = soup.find(class_=\"a-last\")\n",
    "    if next_page_element:\n",
    "        next_page_link = next_page_element.find('a')\n",
    "        if next_page_link:\n",
    "            next_page = 'https://www.amazon.com/' + next_page_link['href']\n",
    "        else:\n",
    "            next_page = None\n",
    "    else:\n",
    "        next_page = None\n",
    "    review_url = next_page\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cae5ce6e-6637-49b0-94eb-506aa0ecaa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A solid follow-up to “Slow Horses”</td>\n",
       "      <td>If you enjoyed “Slow Horses”, then there will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John LeCarré Watch Out!</td>\n",
       "      <td>Second in a delightful spy series in the John ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great thriller</td>\n",
       "      <td>Fun and fast moving the whole way. And funny. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pairweevy</td>\n",
       "      <td>Wonderful characters, ingenious plotting; the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First rate spy thriller 4.5 stars</td>\n",
       "      <td>I started reading this series because I really...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "0  A solid follow-up to “Slow Horses”   \n",
       "1             John LeCarré Watch Out!   \n",
       "2                      great thriller   \n",
       "3                           Pairweevy   \n",
       "4   First rate spy thriller 4.5 stars   \n",
       "\n",
       "                                                body  \n",
       "0  If you enjoyed “Slow Horses”, then there will ...  \n",
       "1  Second in a delightful spy series in the John ...  \n",
       "2  Fun and fast moving the whole way. And funny. ...  \n",
       "3  Wonderful characters, ingenious plotting; the ...  \n",
       "4  I started reading this series because I really...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe from the text data and clean it \n",
    "import pandas as pd\n",
    "\n",
    "text_data = pd.DataFrame(review_data)\n",
    "text_data.dropna(inplace=True)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78a550ca-21d5-4ee4-90fd-a3dc2b00cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get all the strings from each column, and then join them together into one large string\n",
    "#title_list = text_data['title'].to_list()\n",
    "body_list = text_data['body'].to_list()\n",
    "\n",
    "# Transforming the data into a text object\n",
    "\n",
    "# lines = title_list + body_list\n",
    "text = \" \".join(x.strip() for x in body_list)\n",
    "text = re.sub('The media could not be loaded', '', text)\n",
    "\n",
    "# Remove all whitespace using re.sub()\n",
    "pattern = r'[\\t\\n]+'  # '\\t' matches tabs and '\\n' matches newlines, + means more than one\n",
    "text = re.sub(pattern, '', text)\n",
    "\n",
    "emoji = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "text = re.sub(emoji, '', text) # removing all emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "995646cf-438e-4a37-abd7-584e2f984780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the data\n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Expanding contractions\n",
    "text = \" \".join(contractions.fix(word) for word in text.split())\n",
    "\n",
    "# Lowercasing and removing punctuation\n",
    "text = \"\".join(i for i in list(text) if i not in string.punctuation).lower()\n",
    "\n",
    "# Tokenizing the data\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "# Removing the stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "text_words = [word for word in text_tokens if word not in stopwords]\n",
    "\n",
    "# Normalize the text\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "normal_words = []\n",
    "for w in text_words:\n",
    "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "    normal_words.append(word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c9241cc-ac01-4810-9796-346591683371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enjoy': 73, 'slow': 176, 'horse': 167, 'first': 117, 'book': 356, 'bite': 52, 'find': 83, 'second': 83, 'series': 229, 'read': 222, 'love': 55, 'good': 154, 'much': 53, 'author': 63, 'new': 44, 'dead': 74, 'lion': 59, 'story': 135, 'character': 238, 'herron': 137, 'slough': 152, 'house': 147, 'well': 100, 'get': 111, '’': 70, 'end': 74, 'make': 78, 'think': 60, 'come': 47, 'back': 57, 'another': 44, 'one': 126, 'could': 44, 'great': 85, 'spy': 137, 'way': 55, 'british': 42, 'would': 71, 'work': 46, 'plot': 164, 'look': 54, 'start': 57, 'really': 54, 'like': 99, 'agent': 47, 'reader': 58, 'jackson': 98, 'lamb': 151, 'know': 56, 'mi5': 65, 'time': 59, 'mick': 85, 'still': 44, 'novel': 81, 'write': 99, 'go': 61, 'old': 42, 'thriller': 51, 'le': 42, 'keep': 46, 'slow_horse': 155, 'first_book': 42, 'book_series': 37, 'dead_lion': 55, 'slough_house': 141, 'jackson_lamb': 94, 'mick_herron': 66}\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency of each word and combine common adjacent phrases\n",
    "import collections \n",
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "# Calculating the frequency of each pair of words\n",
    "# zip(list, list) combines the two lists into a sequence of tuples, where the first item of each list is paired into a tuple \n",
    "adj_words = zip(normal_words, normal_words[1:]) \n",
    "adj_words_freq = collections.Counter(adj_words)\n",
    "\n",
    "# Getting the max frequency of the pairs \n",
    "max_freq = np.percentile(list(set(adj_words_freq.values())), 75)\n",
    "\n",
    "\n",
    "# Craeting a dictionary of those most frequent pairs \n",
    "pairs = {adj: count for adj, count in adj_words_freq.items() if count > max_freq}\n",
    "pairs_joined = {}\n",
    "for key, value in pairs.items():\n",
    "    pairs_joined[\"_\".join(key)] = value\n",
    "\n",
    "# Getting the most popular solor words based on the median of the freqs\n",
    "s_words = dict(collections.Counter(normal_words))\n",
    "cut_off = statistics.median(set(s_words.values())) - 2\n",
    "solo_words = {word: freq for word, freq in s_words.items() if freq >= cut_off}\n",
    "\n",
    "\n",
    "# Combining both together\n",
    "cloud_list = {}\n",
    "cloud_list.update(solo_words)\n",
    "cloud_list.update(pairs_joined)\n",
    "print(cloud_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a27f90c-05c1-4c4f-bef3-a836a09c6ad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WordCloud.__init__() got an unexpected keyword argument 'background'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generate the word cloud\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m wordcloud\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(cloud_list)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Display the word cloud using matplotlib\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: WordCloud.__init__() got an unexpected keyword argument 'background'"
     ]
    }
   ],
   "source": [
    "# Creating the word cloud\n",
    "# !pip install wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\")\n",
    "wordcloud.generate_from_frequencies(cloud_list)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08aac1-63e8-4c91-9f39-8b537e5c5e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
